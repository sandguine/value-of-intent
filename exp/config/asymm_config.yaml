# General Experiment Settings
SEED: 30                              # Random seed for reproducibility
NUM_SEEDS: 16                        # Default for hyperparameter tuning (use 100 for adaptability tests)

# Environment Settings
ENV_NAME: "overcooked"                # Environment used for training
ENV_KWARGS: 
  layout : "asymm_advantages"          # Overcooked layout: cramped_room, asymm_advantages, counter_circuit, coord_ring, forced_coord

ACTIVATION: "tanh"                    # Activation function for the network (tanh or relu)

# PPO Hyperparameters
LR: 0.0001                            # changed from 2.5e-4 to 0.0001
ANNEAL_LR: true                       # Whether to anneal the learning rate linearly over time
NUM_ENVS: 128                         # Number of parallel environments for sample collection
NUM_STEPS: 128                        # Number of steps per environment before updating the policy
TOTAL_TIMESTEPS: 5e7                  # Total timesteps to run the experiment (increased from previous)
UPDATE_EPOCHS: 4                      # Number of update epochs per PPO iteration
NUM_MINIBATCHES: 4                    # Number of minibatches to split each batch during training
GAMMA: 0.98                           # Discount factor for rewards
GAE_LAMBDA: 0.95                      # Lambda for Generalized Advantage Estimation
CLIP_EPS: 0.1                         # Clipping epsilon for PPO
ENT_COEF: 0.01                        # Entropy regularization coefficient (test between 0.01 and 0.05)
VF_COEF: 0.5                          # Coefficient for value function loss
MAX_GRAD_NORM: 0.5                    # Maximum gradient norm for clipping
REW_SHAPING_HORIZON: 2.5e7            # Horizon for reward shaping

# WandB Integration
ENTITY: "sandily"                     # WandB entity name (organization or user)
PROJECT: "test-adapt"                 # WandB project name
WANDB_MODE: "online"                  # WandB mode (online, offline, disabled)
